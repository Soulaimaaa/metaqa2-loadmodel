{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ukp-storage-1/khammari/miniconda3/envs/metaa/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler,DefaultDataCollator, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
    "from datasets import load_dataset,load_metric\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.python_logger import NeptuneHandler\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchsummary import summary \n",
    "\n",
    "path_for_config='/ukp-storage-1/khammari/QA-Verification-Via-NLI/MetaQA2/config.yaml'\n",
    "with open(path_for_config, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "predictions_path = config['paths']['train_data_path']\n",
    "eval_path = config['paths']['dev_data_path']\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    filename=\"/ukp-storage-1/khammari/log-train_model_with_LR_13_08_2.txt\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question_statement_text\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=False,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    inputs[\"label\"] = [1 if label else 0 for label in examples['label']]\n",
    "    return inputs\n",
    "\n",
    "def compute_metrics(preds,labels):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "def get_f1_EM(eval_data,agents,predictions):\n",
    "        qids = eval_data[\"id\"]\n",
    "        # only the probability that the hypothesis and the premise are entailed is retrieved\n",
    "        probs_after_softmax = [torch.nn.Softmax(dim=-1)(torch.tensor(p)).tolist() for p in predictions]\n",
    "        entail_pred = [p[1] for p in probs_after_softmax]\n",
    "        # map the probabilties of each agent depending on qid\n",
    "        mappings_agents ={agents[0]:{},agents[1]:{}}\n",
    "\n",
    "        for i in range(len(qids)):\n",
    "            agent = eval_data['agent'][i]\n",
    "            qid = qids[i]\n",
    "            gold_ans = eval_data['golden_answer'][i]\n",
    "            pred = eval_data['answer_text'][i]\n",
    "            prob_of_entailement = entail_pred[i]\n",
    "            ########################################\n",
    "            mappings_agents[agent][qid]={\"answer_text\":pred,\"golden_answer\":gold_ans,\"entails_pred\":prob_of_entailement}\n",
    "        \n",
    "        probAgents_forargmax = []\n",
    "        ids = []\n",
    "        for id in qids:\n",
    "            if id not in ids:\n",
    "                probAgents_forargmax.append([mappings_agents[agents[0]][id][\"entails_pred\"],mappings_agents[agents[1]][id][\"entails_pred\"]])\n",
    "                ids.append(id)\n",
    "        bestAgent = torch.argmax(torch.Tensor(probAgents_forargmax) , dim=1)    \n",
    "\n",
    "        ##############################################################################\n",
    "        # 1) load the metric\n",
    "        metric = load_metric('squad')\n",
    "        # 2) get the list of labels in the format of the squad metric\n",
    "        references = []\n",
    "        predictions = []\n",
    "        for i in range(len(ids)):\n",
    "            qid = ids[i]\n",
    "            ref = {'id': qid , 'answers': {'text': [], 'answer_start': []}} \n",
    "            for ans in [mappings_agents[agents[bestAgent[i]]][qid][\"golden_answer\"]]:\n",
    "                ref['answers']['text'].append(ans)\n",
    "                ref['answers']['answer_start'].append(0)\n",
    "            references.append(ref)\n",
    "            # 3) get the predictions in the format of the squad metric\n",
    "            if (mappings_agents[agents[bestAgent[i]]][qid][\"entails_pred\"]<0.5):\n",
    "                ans = \"\"\n",
    "            else:\n",
    "                ans = mappings_agents[agents[bestAgent[i]]][qid][\"answer_text\"]\n",
    "            predictions.append({'id': qid, 'prediction_text': ans})\n",
    "        # 4) evaluate the predictions\n",
    "        results = metric.compute(predictions=predictions, references=references)\n",
    "        return results[\"exact_match\"], results[\"f1\"]\n",
    "\n",
    "def evaluate(preds,labels,b,agents,logits):\n",
    "    acc= compute_metrics(preds,labels)[\"accuracy\"]\n",
    "    em,f1=  get_f1_EM(b,agents,logits)\n",
    "    preds =  np.argmax([l.tolist() for l in logits], axis=-1)\n",
    "    return acc, em, f1, preds\n",
    "\n",
    "\n",
    "def mycollator(batch):\n",
    "    result = {}\n",
    "    for k in batch[0].keys():\n",
    "        if k == \"labels\":\n",
    "            result[k] = torch.tensor([1 if x[k] else 0 for x in batch])\n",
    "        elif k in [\"answer_text\",\"golden_answer\",\"id\",\"agent\"]:\n",
    "            result[k] = [x[k] for x in batch]\n",
    "        else:\n",
    "            result[k] = torch.tensor([x[k] for x in batch])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/soulaima/fine-tuning/e/FIN-82\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 113.26it/s]\n",
      "100%|██████████| 8/8 [00:04<00:00,  1.73ba/s]\n",
      "/ukp-storage-1/khammari/miniconda3/envs/metaa/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 1/1250 [02:06<43:44:48, 126.09s/it]\n",
      "Iteration:   0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 0, 1, 1, 0, 0, 0, 1]\n",
      "see loss  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ukp-storage-1/khammari/miniconda3/envs/metaa/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "\n",
      "Iteration:   0%|          | 0/973 [02:04<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1250 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 35 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 35 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/soulaima/fine-tuning/e/FIN-82\n"
     ]
    }
   ],
   "source": [
    "#  Must be a list of str. Tags of the run. They are editable after run is created. \n",
    "# Tags are displayed in the run's Details and can be viewed in Runs table view as a column.\n",
    "run = neptune.init(tags=[\"triviaQA+SearchQA\"], project=\"soulaima/fine-tuning\",\n",
    "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNzZlYzYxYy04YTMzLTQxNjctYjM3ZS0zZGEyOTJhMWQ2ZWYifQ==\",\n",
    ")\n",
    "\n",
    "logger.addHandler(NeptuneHandler(run=run))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# todo: change hardcoding params and use a yaml file for config (see metaqa repo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased').to(device)\n",
    "dataset = load_dataset('json', data_files=predictions_path)\n",
    "evalDataset = load_dataset('json', data_files=eval_path)\n",
    "agents = [\"spanbert-large-cased_SearchQA\",\"spanbert-large-cased_TriviaQA-web\"]\n",
    "\n",
    "tokenized_input = dataset.map(preprocess_function, batched=True, remove_columns=['id', 'question_text', 'answer_text', 'agent','golden_answer','context', 'dataset', 'question_statement_text'])\n",
    "tokenized_eval_input = evalDataset.map(preprocess_function, batched=True, remove_columns=['question_text', 'context', 'dataset', 'question_statement_text'])\n",
    "tokenized_input = tokenized_input.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval_input = tokenized_eval_input.rename_column(\"label\", \"labels\")\n",
    "\n",
    "#tokenized_input.with_format(\"torch\")\n",
    "#tokenized_eval_input.with_format(\"torch\", columns=[\"labels\"], output_all_columns=True) \n",
    "#tokenized_eval_input.set_format(\"torch\")\n",
    "#data_collator = DefaultDataCollator()\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_input[\"train\"], shuffle=True, batch_size=8, collate_fn=mycollator)\n",
    "eval_dataloader = DataLoader(tokenized_eval_input[\"train\"], batch_size=8, collate_fn=mycollator)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=config['model_config']['num_epochs'] * len(train_dataloader))\n",
    "progress_bar = tqdm(range(config['model_config']['num_epochs'] * len(train_dataloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Training...')\n",
    "\n",
    "for epoch in range(config['model_config']['num_epochs']): # should be 1\n",
    "    # model.train() sets the mode to train\n",
    "    model.train()\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch[\"input_ids\"],labels= batch['labels'],attention_mask=batch[\"attention_mask\"])\n",
    "        \n",
    "        logits_try = outputs.logits\n",
    "        predictions_try = torch.argmax(logits_try, dim=-1)\n",
    "        i = 0\n",
    "        test_preds = predictions_try.tolist()\n",
    "        test_labels = batch['labels'].tolist()\n",
    "        print(test_preds)\n",
    "        print(test_labels)\n",
    "        for j in range(len(test_labels)):\n",
    "            if test_labels[j] != test_preds[j]:\n",
    "                i +=1\n",
    "        print(\"see loss \",i/len(test_labels))\n",
    "        # vgg = model.vgg16()\n",
    "        # print(summary(vgg,(3,224,224)))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # log in the loss\n",
    "        run[\"train/epoch/loss\"].log(loss.data.item())\n",
    "        # log in the learning rate (lr)\n",
    "        run[\"train/epoch/lr\"].log(lr_scheduler.get_lr()[0])\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # evaluate on the evaluation set every X training steps\n",
    "        epoch_iterator_eval = tqdm(eval_dataloader, desc=\"Iteration\")\n",
    "        # either model.eval() or model.train(mode=False) to tell that you are testing\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        em = 0\n",
    "        f1 = 0\n",
    "        preds = 0\n",
    "        # all_predictions = []\n",
    "        # all_references = []\n",
    "        if eval_step % config['model_config']['eval_every'] == 0:\n",
    "            for eval_step, eval_batch in enumerate(epoch_iterator_eval):\n",
    "                eval_batch_for_pred = {k: v.to(device) if k not in [\"answer_text\",\"golden_answer\",\"id\",\"agent\"] else v for k, v in eval_batch.items() } \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids=eval_batch_for_pred[\"input_ids\"],labels=eval_batch_for_pred['labels'],attention_mask=eval_batch_for_pred[\"attention_mask\"])\n",
    "\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                logger.info(f\"Evaluating on dev set at step {eval_step}\")\n",
    "                acc_temp, em_temp, f1_temp, preds_temp = evaluate(predictions,eval_batch_for_pred[\"labels\"],eval_batch,agents,logits) \n",
    "                acc += acc_temp\n",
    "                em += em_temp\n",
    "                f1 += f1_temp\n",
    "                preds += preds_temp\n",
    "            run[f\"dev/eval/acc\"].log(round(acc*100, 2))\n",
    "            run[f\"dev/eval/em\"].log(em)\n",
    "            run[f\"dev/eval/f1\"].log(f1)\n",
    "            run[\"dev/eval/preds\"].log(preds.tolist())\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('metaa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b271db6cda0a5132ea42f1609c9da755a5fea436dd55a335fc73ba0f3be52f01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
